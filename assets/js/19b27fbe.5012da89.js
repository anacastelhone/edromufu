"use strict";(self.webpackChunkedrom=self.webpackChunkedrom||[]).push([[679],{3905:(e,n,o)=>{o.d(n,{Zo:()=>l,kt:()=>f});var a=o(7294);function t(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function r(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),o.push.apply(o,a)}return o}function i(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?r(Object(o),!0).forEach((function(n){t(e,n,o[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))}))}return e}function s(e,n){if(null==e)return{};var o,a,t=function(e,n){if(null==e)return{};var o,a,t={},r=Object.keys(e);for(a=0;a<r.length;a++)o=r[a],n.indexOf(o)>=0||(t[o]=e[o]);return t}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)o=r[a],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(t[o]=e[o])}return t}var c=a.createContext({}),d=function(e){var n=a.useContext(c),o=n;return e&&(o="function"==typeof e?e(n):i(i({},n),e)),o},l=function(e){var n=d(e.components);return a.createElement(c.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},p=a.forwardRef((function(e,n){var o=e.components,t=e.mdxType,r=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=d(o),p=t,f=u["".concat(c,".").concat(p)]||u[p]||m[p]||r;return o?a.createElement(f,i(i({ref:n},l),{},{components:o})):a.createElement(f,i({ref:n},l))}));function f(e,n){var o=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var r=o.length,i=new Array(r);i[0]=p;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s[u]="string"==typeof e?e:t,i[1]=s;for(var d=2;d<r;d++)i[d]=o[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,o)}p.displayName="MDXCreateElement"},8037:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var a=o(7462),t=(o(7294),o(3905));const r={id:"running_inference.py",title:"running_inference.py",desrcion:"Nesta se\xe7\xe3o teremos um explica\xe7\xe3o sobre o c\xf3digo running_inference.py",slug:"/running_inference",sidebar_position:3},i=void 0,s={unversionedId:"vision/codes/running_inference.py",id:"vision/codes/running_inference.py",title:"running_inference.py",description:"Nesta se\xe7\xe3o teremos um explica\xe7\xe3o detalhada sobre o c\xf3digo running_inference.py",source:"@site/docs/vision/codes/running_inference.md",sourceDirName:"vision/codes",slug:"/running_inference",permalink:"/edromufu/docs/running_inference",draft:!1,editUrl:"https://github.com/edromufu/edromufu/tree/master/edrom-docs/docs/vision/codes/running_inference.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"running_inference.py",title:"running_inference.py",desrcion:"Nesta se\xe7\xe3o teremos um explica\xe7\xe3o sobre o c\xf3digo running_inference.py",slug:"/running_inference",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"connecting_and_showing.py",permalink:"/edromufu/docs/connecting_and_showing"},next:{title:"Instala\xe7\xe3o CUDA e cuDNN",permalink:"/edromufu/docs/cuda-cudnn"}},c={},d=[{value:"get_cnn_files()",id:"get_cnn_files",level:2},{value:"read_cnn_architecture()",id:"read_cnn_architecture",level:2},{value:"Set_model_input()",id:"set_model_input",level:2},{value:"Detect_model()",id:"detect_model",level:2},{value:"Draw_results()",id:"draw_results",level:2}],l={toc:d},u="wrapper";function m(e){let{components:n,...o}=e;return(0,t.kt)(u,(0,a.Z)({},l,o,{components:n,mdxType:"MDXLayout"}),(0,t.kt)("p",null,"Nesta se\xe7\xe3o teremos um explica\xe7\xe3o detalhada sobre o c\xf3digo running_inference.py"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},"#!/usr/bin/env python3\n# coding=utf-8\n\nimport os\nimport cv2\nimport numpy as np\nimport time\n\n\noutline_color_list = [(255, 0, 0), (0, 0, 255), (0, 0, 255)]\n")),(0,t.kt)("p",null,"Nesse c\xf3digo temos algumas importa\xe7\xf5es:"),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"\u201cos\u201d = Biblioteca que possui algumas fun\xe7\xf5es que imitam as do sistema operacional")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"\u201ccv2\u201d = OpenCV, Biblioteca para trabalhar com imagens")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"\u201cNumpy\u201d = Biblioteca para o processamento de grandes n\xfameros, vetores e matrizes.")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"\u201ctime\u201d = Biblioteca que trabalha que possui v\xe1rias fun\xe7\xf5es referentes ao tempo."))),(0,t.kt)("h2",{id:"get_cnn_files"},"get_cnn_files()"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},'def get_cnn_files():\n\n    robocup_folder = os.path.join(os.path.expanduser(\'~\'), "vis\xe3o/robocup_cnn_files")\n\n    config_file = os.path.join(robocup_folder, "yolov4-tiny-obj.cfg")\n    weights_file = os.path.join(robocup_folder, "yolov4-tiny-obj_best.weights")\n\n    return read_cnn_architecture(config_file, weights_file)\n')),(0,t.kt)("p",null,'Esta fun\xe7\xe3o recupera os nomes dos arquivos de uma rede neural convolucional (CNN) especificando as rotas dos arquivos de configura\xe7\xe3o e pesos. A fun\xe7\xe3o usa a biblioteca os para juntar o caminho do diret\xf3rio home do usu\xe1rio com a pasta "vis\xe3o/robocup_cnn_files" espec\xedfica onde os arquivos est\xe3o localizados. Os arquivos espec\xedficos dentro desta pasta s\xe3o "yolov4-tiny-obj.cfg" e "yolov4-tiny-obj_best.weights". A fun\xe7\xe3o ent\xe3o retorna a sa\xedda da fun\xe7\xe3o ',(0,t.kt)("em",{parentName:"p"},"read_cnn_architecture()"),", passando os arquivos config_file e weights_file como argumentos."),(0,t.kt)("h2",{id:"read_cnn_architecture"},"read_cnn_architecture()"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},'def read_cnn_architecture(config_file, weights_file):\n\n    net = cv2.dnn.readNet(config_file, weights_file, "darknet")\n\n    return net\n')),(0,t.kt)("p",null,"Esta fun\xe7\xe3o l\xea a arquitetura e os pesos de uma rede neural convolucional (CNN) usando a biblioteca OpenCV. Ele recebe dois argumentos, o caminho do arquivo de configura\xe7\xe3o e o caminho do arquivo de pesos. A fun\xe7\xe3o, ent\xe3o, usa a fun\xe7\xe3o ",(0,t.kt)("em",{parentName:"p"},"cv2.dnn.readNet()"),' da biblioteca OpenCV para carregar a rede com o config_file e o weights_file dados e passando a string "darknet" como terceiro argumento. Esta fun\xe7\xe3o retorna o modelo CNN carregado representado pela vari\xe1vel "net".'),(0,t.kt)("h2",{id:"set_model_input"},"Set_model_input()"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},"def set_model_input(net):\n\n    model = cv2.dnn.DetectionModel(net)\n    model.setInputParams(size=(416,416), scale=1/255, swapRB=True)\n    \n    return model\n")),(0,t.kt)("p",null,"Esta fun\xe7\xe3o define a entrada do modelo de uma rede neural convolucional (CNN) usando a biblioteca OpenCV. Ela recebe um argumento, que \xe9 o modelo da rede neural carregado anteriormente (net). A fun\xe7\xe3o cria uma vari\xe1vel model, que \xe9 uma inst\xe2ncia da classe ",(0,t.kt)("em",{parentName:"p"},"cv2.dnn.DetectionModel")," e passa a rede neural carregada (net) como argumento. Em seguida, \xe9 utilizado o m\xe9todo setInputParams da classe model para definir os par\xe2metros de entrada da rede neural, que s\xe3o o tamanho da imagem de entrada (416x416 pixels), a escala dos valores de pixel (1/255) e a troca do canal de cor vermelho e azul (swapRB=True). Por fim, a fun\xe7\xe3o retorna o modelo configurado."),(0,t.kt)("h2",{id:"detect_model"},"Detect_model()"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},'def detect_model(model, current_frame):\n    \n    start_time = time.time()\n    classes, scores, boxes = model.detect(current_frame, 0.85, 0.4)\n    finish_time = time.time()\n    fps = 1/(finish_time-start_time)\n    \n    #print(f"Classes: {classes}, Scores: {scores}")\n    #print(f"Boxes: {boxes}")\n    #print("FPS: ", fps)\n    #print(\'\\n\')\n\n    return classes, scores, boxes, int(fps)\n')),(0,t.kt)("p",null,"Esta fun\xe7\xe3o detecta objetos em uma imagem usando um modelo de rede neural convolucional (CNN) previamente configurado. Ela recebe dois argumentos, o modelo configurado (model) e a imagem atual (current_frame). A fun\xe7\xe3o inicia uma contagem de tempo, chamando a fun\xe7\xe3o ",(0,t.kt)("em",{parentName:"p"},"time.time()"),", para medir o desempenho do modelo. Ent\xe3o, a fun\xe7\xe3o usa o m\xe9todo ",(0,t.kt)("em",{parentName:"p"},"detect()")," do modelo para detectar objetos na imagem atual e obt\xe9m tr\xeas vari\xe1veis de sa\xedda: classes (as classes dos objetos detectados), scores (a confian\xe7a das detec\xe7\xf5es) e boxes (as caixas delimitadoras dos objetos detectados). Os par\xe2metros 0.85 e 0.4 passados para ",(0,t.kt)("em",{parentName:"p"},"detect()")," s\xe3o respectivamente o limiar de confian\xe7a m\xednima e o limiar de maxima supress\xe3o. Em seguida, a fun\xe7\xe3o calcula a taxa de quadros por segundo (FPS) como 1 dividido pela diferen\xe7a entre o tempo final e o tempo inicial. A fun\xe7\xe3o retorna as classes, scores, boxes e FPS."),(0,t.kt)("h2",{id:"draw_results"},"Draw_results()"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="object_finder/src/running_inference.py"',title:'"object_finder/src/running_inference.py"'},'def draw_results(frame, classes, scores, boxes):\n\n    # Draw the bounding boxes\n\n    for i in range(len(boxes)):\n        [x_top, y_top, roi_width, roi_height] = boxes[i]\n        #p1 = (x_top, y_top)\n        #p2 = (x_top + roi_width, y_top + roi_height)\n        p3 = (x_top, y_top - 5)\n\n        x_center = round(x_top+(roi_width/2))\n        y_center = round(y_top+(roi_height/2))\n        radius = round((roi_width+roi_height)/4)\n        \n        #cv2.rectangle(frame, p1, p2, outline_color_list[classes[i]], 2)\n        cv2.circle (frame,(x_center,y_center),radius, outline_color_list[classes[i]],2 )\n        confidence = str(round(float(scores[i]), 2))\n        \n        label = "Ball"\n\n        cv2.putText(frame, label+" " + confidence, p3, cv2.FONT_HERSHEY_PLAIN, 1, (255,255,255), 1)\n')),(0,t.kt)("p",null,'Esta fun\xe7\xe3o desenha as detec\xe7\xf5es de objetos em uma imagem. Ela recebe quatro argumentos: a imagem (frame), as classes dos objetos detectados (classes), as confian\xe7as das detec\xe7\xf5es (scores) e as caixas delimitadoras dos objetos detectados (boxes). A fun\xe7\xe3o usa um loop para percorrer todas as caixas delimitadoras e, para cada uma, extrai as coordenadas x e y do canto superior esquerdo, bem como a largura e a altura da caixa. Em seguida, calcula o centro da caixa e o raio. Em vez de desenhar uma caixa em volta do objeto detectado, o c\xf3digo desenha um c\xedrculo no centro do objeto com o raio calculado. Em seguida, o c\xf3digo adiciona o texto "Bola" junto com a confian\xe7a da detec\xe7\xe3o na imagem, logo acima do objeto detectado. A fun\xe7\xe3o n\xe3o retorna nenhum valor, pois modifica a imagem passada como argumento.'))}m.isMDXComponent=!0}}]);